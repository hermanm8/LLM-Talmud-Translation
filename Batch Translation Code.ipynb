{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "class OpenAIWrapper:\n",
    "    def __init__(self, client, model_name=\"gpt-5\", temperature=0.1):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"model\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"system_prompt\": \"You are a Talmud scholar expert in translation.\"\n",
    "        }\n",
    "\n",
    "def daf_amud_generator():\n",
    "    \"\"\"Generator for Daf/Amud references (e.g., 2a, 2b, 3a, 3b)\"\"\"\n",
    "    daf = 2\n",
    "    amud = 'a'\n",
    "    while True:\n",
    "        yield f\"{daf}{amud}\"\n",
    "        if amud == 'a':\n",
    "            amud = 'b'\n",
    "        else: # amud == 'b'\n",
    "            amud = 'a'\n",
    "            daf += 1\n",
    "\n",
    "def strip_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Placeholderâ€”actual cleaning is done during retrieval\n",
    "def clean_translation_output(text):\n",
    "    return text.strip() \n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. BATCH PREPARATION LOGIC (Top Level Function)\n",
    "# ==============================================================================\n",
    "\n",
    "def prepare_batch_file(full_tractate_data, promt_template, batch_file_path, model_name, book_identifier, is_test):\n",
    "    \"\"\"\n",
    "    Prepares a .jsonl file for the OpenAI Batch API.\n",
    "    book_identifier: The clean name of the tractate (e.g., 'Bekhorot').\n",
    "    Returns: total number of requests generated, full batch file path.\n",
    "    \"\"\"\n",
    "    all_hebrew_pages = full_tractate_data.get('hebrew', [])\n",
    "    all_english_pages = full_tractate_data.get('english', [])\n",
    "    passages_to_process = list(zip(all_hebrew_pages, all_english_pages))\n",
    "\n",
    "    if is_test:\n",
    "        passages_to_process = passages_to_process[:2]\n",
    "        \n",
    "    daf_amud_gen = daf_amud_generator()\n",
    "    requests_generated = 0\n",
    "    batch_requests = []\n",
    "    system_prompt = \"You are a Talmud scholar expert in translation.\" \n",
    "\n",
    "    try:\n",
    "        for hebrew_page, _ in passages_to_process:\n",
    "            daf_amud_key = next(daf_amud_gen)\n",
    "            \n",
    "            if isinstance(hebrew_page, str): hebrew_page = [hebrew_page]\n",
    "            \n",
    "            for line_idx, hebrew_line in enumerate(hebrew_page):\n",
    "                hebrew_line_clean = strip_html(hebrew_line)\n",
    "\n",
    "                if not hebrew_line_clean.strip():\n",
    "                    continue\n",
    "\n",
    "                user_content = promt_template.format(text=hebrew_line_clean)\n",
    "                \n",
    "                # --- Custom ID Format: [BOOK]_[DAF_AMUD]_[LINE_IDX] ---\n",
    "                # Example: 'Bekhorot_2a_1'\n",
    "                custom_id = f\"{book_identifier}_{daf_amud_key}_{line_idx+1}\"\n",
    "                \n",
    "                batch_request = {\n",
    "                    \"custom_id\": custom_id, \n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        \"model\": model_name,\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_content}\n",
    "                        ],\n",
    "                        \"temperature\": 0.1, \n",
    "                        \"max_completion_tokens\": 1024 \n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                batch_requests.append(batch_request)\n",
    "                requests_generated += 1\n",
    "\n",
    "        # Write all requests to the .jsonl file\n",
    "        with open(batch_file_path, 'w', encoding='utf-8') as f:\n",
    "            for req in batch_requests:\n",
    "                f.write(json.dumps(req) + '\\n')\n",
    "\n",
    "        print(f\"  > Prepared {requests_generated} requests.\")\n",
    "        return requests_generated, batch_file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing batch file: {e}\")\n",
    "        return 0, None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. BATCH SUBMISSION LOGIC (Top Level Function)\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_translations_batch_submit(\n",
    "        llm, \n",
    "        promt_template, \n",
    "        file_name_prefix, \n",
    "        input_file_path, \n",
    "        is_test=True):\n",
    "    \n",
    "    # 1. Determine file names and identifiers\n",
    "    base_file_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "    starttime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    batch_dir = \"batch_input_files\"\n",
    "    os.makedirs(batch_dir, exist_ok=True)\n",
    "    batch_file_path = f\"{batch_dir}/{file_name_prefix}{base_file_name}_{starttime}.jsonl\"\n",
    "    \n",
    "    model_name = llm.model_name\n",
    "    book_identifier = base_file_name # e.g., 'Bekhorot'\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            full_tractate_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"  > Error loading input data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. PREPARE INPUT FILE (Calls the helper function)\n",
    "    requests_generated, input_path = prepare_batch_file(\n",
    "        full_tractate_data, \n",
    "        promt_template, \n",
    "        batch_file_path, \n",
    "        model_name,\n",
    "        book_identifier, \n",
    "        is_test\n",
    "    )\n",
    "\n",
    "    if requests_generated == 0:\n",
    "        print(\"  > No requests generated. Aborting batch submission.\")\n",
    "        return None\n",
    "\n",
    "    # 3. UPLOAD FILE\n",
    "    try:\n",
    "        print(f\"  > Uploading file: {os.path.basename(input_path)}\")\n",
    "        batch_input_file = llm.client.files.create(\n",
    "            file=open(input_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        print(f\"  > File uploaded successfully. File ID: {batch_input_file.id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  > Error uploading file to OpenAI: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 4. CREATE BATCH JOB\n",
    "    try:\n",
    "        batch_job = llm.client.batches.create(\n",
    "            input_file_id=batch_input_file.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "\n",
    "        print(f\"  > BATCH JOB SUBMITTED:\")\n",
    "        print(f\"    Job ID: {batch_job.id}\")\n",
    "        print(f\"    Status: {batch_job.status}\")\n",
    "        print(f\"    Total Requests: {batch_job.request_counts.total}\")\n",
    "\n",
    "        return batch_job.id\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  > Error creating batch job: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION BLOCK (Orchestration)\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "    INPUT_DIR = './talmud_output'\n",
    "    \n",
    "    # Ensure VADER is downloaded\n",
    "    try:\n",
    "        nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "    except nltk.downloader.DownloadError:\n",
    "        nltk.download('vader_lexicon')\n",
    "\n",
    "    # Instatiate model client \n",
    "    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "    llm = OpenAIWrapper(client, model_name=\"gpt-3.5-turbo\")\n",
    "   \n",
    "    prompt = PromptTemplate(\n",
    "                     input_variables=[\"text\"],\n",
    "                     template=\"\"\"You are a Talmud scholar translating a tractate of the Talmud that contains both Hebrew and Aramaic.\n",
    "                         Translate the following text into English word-for-word, maintaining the original style and format of Talmudic discourse.\n",
    "                         Keep names and technical terms transliterated.\n",
    "                         Preserve any quotes from Biblical verses and translate them.\n",
    "                         Do not add any commentary or explanations. Your readers are also Talmud Scholars and do not require any Notes.\n",
    "\n",
    "                         Talmudic text: {text}\n",
    "\n",
    "                         English translation:\"\"\"\n",
    "                 )\n",
    "    promt_template = prompt.template\n",
    "    \n",
    "    submitted_jobs = {} \n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(INPUT_DIR):\n",
    "        input_file_path = os.path.join(INPUT_DIR, filename)\n",
    "\n",
    "        # Skip directories and non-JSON files for safety\n",
    "        if not filename.endswith('.json') or os.path.isdir(input_file_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Processing {filename} ---\")\n",
    "            \n",
    "        try:\n",
    "            job_id = analyze_translations_batch_submit(\n",
    "                llm,\n",
    "                promt_template,\n",
    "                file_name_prefix = \"gpt-3.5_batch_\",\n",
    "                input_file_path = input_file_path,\n",
    "                is_test=False \n",
    "            )\n",
    "                \n",
    "            if job_id:\n",
    "                submitted_jobs[os.path.basename(input_file_path)] = {\n",
    "                    \"job_id\": job_id,\n",
    "                    \"input_file_path\": input_file_path \n",
    "                }\n",
    "\n",
    "            pause_time_seconds = 600\n",
    "            print(f\"\\n--- PAUSING for {pause_time_seconds / 60} minutes (10 min) before next tractate... ---\")\n",
    "            time.sleep(pause_time_seconds)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {filename}: {str(e)}\")\n",
    "                \n",
    "    \n",
    "    # --- FINAL STEP: Save Job IDs ---\n",
    "    if submitted_jobs:\n",
    "        print(\"\\n--- All submissions complete. Saving Job IDs for retrieval. ---\")\n",
    "        \n",
    "        metadata_dir = \"batch_metadata\"\n",
    "        os.makedirs(metadata_dir, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        metadata_file = os.path.join(metadata_dir, f\"submitted_jobs_metadata_{timestamp}.json\")\n",
    "        \n",
    "        final_metadata = {\n",
    "            \"submission_time\": timestamp,\n",
    "            \"model_config\": llm.get_config(),\n",
    "            \"jobs\": submitted_jobs\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump(final_metadata, f, indent=4)\n",
    "            print(f\"âœ… SUCCESS: Job IDs saved to: {metadata_file}\")\n",
    "            print(\"\\nREMINDER: Wait 1-24 hours, then run the retrieval script.\")\n",
    "        except Exception as e:\n",
    "            print(f\"FATAL ERROR: Could not save metadata file! Error: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo jobs were successfully submitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def combine_csv_files(folder_path, output_filename=\"combined_data.csv\"):\n",
    "    \"\"\"\n",
    "    Combines all CSV files within a specified folder into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the CSV files.\n",
    "        output_filename (str): The name of the resulting combined CSV file.\n",
    "    \"\"\"\n",
    "    # 1. Define the search pattern\n",
    "    # The glob module finds all pathnames matching a specified pattern.\n",
    "    search_pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob(search_pattern)\n",
    "\n",
    "    # Check if any CSV files were found\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files to combine.\")\n",
    "\n",
    "    # 2. Read and store all dataframes\n",
    "    all_data = []\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            # Read the CSV file. We assume they all have the same columns.\n",
    "            # You might need to adjust parameters like 'encoding' or 'sep'\n",
    "            # if your files are not standard UTF-8/comma-separated.\n",
    "            df = pd.read_csv(f)\n",
    "            all_data.append(df)\n",
    "            print(f\"  - Successfully read: {os.path.basename(f)} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - ERROR reading {os.path.basename(f)}: {e}\")\n",
    "\n",
    "    # 3. Concatenate all dataframes\n",
    "    if all_data:\n",
    "        try:\n",
    "            # pd.concat stacks the DataFrames on top of each other\n",
    "            combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "            # 4. Write the combined dataframe to a new CSV file\n",
    "            output_path = os.path.join(os.getcwd(), output_filename)\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "            print(\"\\n--- COMBINATION COMPLETE ---\")\n",
    "            print(f\"Total rows in combined file: {len(combined_df)}\")\n",
    "            print(f\"Output saved to: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- ERROR during combination/saving ---\")\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print(\"No data was successfully read to combine.\")\n",
    "\n",
    "# --- USAGE INSTRUCTIONS ---\n",
    "# 1. If your CSV files are in the same folder as this Jupyter notebook:\n",
    "#    folder_to_process = \".\"\n",
    "\n",
    "# 2. If your CSV files are in a subfolder named 'data':\n",
    "#    folder_to_process = \"data\"\n",
    "\n",
    "# 3. For an absolute path (replace with your actual folder path):\n",
    "#    folder_to_process = \"/Users/username/Desktop/my_csv_files\"\n",
    "\n",
    "\n",
    "# >>> Set your folder path here <<<\n",
    "folder_to_process = \"analysis_output\"\n",
    "\n",
    "# Execute the function\n",
    "combine_csv_files(folder_to_process, output_filename=\"all_combined_gpt35_translations.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
