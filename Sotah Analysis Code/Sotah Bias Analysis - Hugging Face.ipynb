{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run for all imports and to instantiate Hugging Face Wrapper\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sacrebleu.metrics import BLEU, TER\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "bleu  = BLEU(effective_order=True)\n",
    "ter   = TER()\n",
    "comet_ckpt = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_ckpt).eval()\n",
    "\n",
    "model_id = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class HuggingFaceWrapper:\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "        try:\n",
    "            # Hugging Face pipelines expect a plain string prompt\n",
    "            result = self.pipe(prompt, max_new_tokens=512, do_sample=False)\n",
    "            output = result[0]['generated_text']\n",
    "\n",
    "            # Fix 3: Remove prompt echo if present\n",
    "            if output.startswith(prompt):\n",
    "                output = output[len(prompt):].strip()\n",
    "\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Hugging Face model call: {str(e)}\")\n",
    "            return \"\"\n",
    "    def get_config(self):\n",
    "        model_config = getattr(self.pipe.model, 'config', None)\n",
    "        model_name = model_config.name_or_path if model_config else \"Unknown\"\n",
    "        device = self.pipe.device if hasattr(self.pipe, 'device') else \"Unknown\"\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"framework\": \"transformers\",\n",
    "            \"device\": str(device),\n",
    "            \"tokenizer\": type(self.pipe.tokenizer).__name__,\n",
    "            \"model_class\": type(self.pipe.model).__name__,\n",
    "            \"pipeline_type\": self.pipe.task\n",
    "        }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run for Sotah translation and translation metrics\n",
    "\n",
    "def truncate_to_max_tokens(prompt: str, max_tokens: int = 4096):\n",
    "    tokens = tokenizer(prompt, truncation=False)[\"input_ids\"]\n",
    "    was_truncated = len(tokens) > max_tokens\n",
    "    if not was_truncated:\n",
    "        return prompt, False\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_prompt = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    return truncated_prompt, True\n",
    "\n",
    "def get_passage_analysis(llm, text):\n",
    "    \"\"\"\n",
    "    Get LLM analysis of passage meaning\n",
    "    \n",
    "    Args:\n",
    "        llm: LLM instance\n",
    "        text: Text to analyze\n",
    "        is_translation: Boolean indicating if this is a translation (affects prompt)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"Please analyze this passage and explain:\n",
    "1. What is the main topic or subject being discussed?\n",
    "2. What are the key arguments or points being made?\n",
    "3. Who are the people discussed or mentioned?\n",
    "\n",
    "Passage: {text}\n",
    "\"\"\")\n",
    "    \n",
    "    analysis = llm.invoke(prompt_template.format(text=text))\n",
    "    \n",
    "    return str(analysis).strip()\n",
    "\n",
    "def compare_analyses(analysis1, analysis2):\n",
    "    \"\"\"\n",
    "    Compare two passage analyses using similarity metrics\n",
    "    \n",
    "    Args:\n",
    "        analysis1: First analysis text\n",
    "        analysis2: Second analysis text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing comparison metrics\n",
    "    \"\"\"\n",
    "    similarity = similarity_score(analysis1, analysis2)\n",
    "    \n",
    "    return {\n",
    "        'similarity_score': similarity,\n",
    "        'original_analysis': analysis1,\n",
    "        'translation_analysis': analysis2,\n",
    "        'prompt': \"\"\"Please analyze this passage and explain:\n",
    "1. What is the main topic or subject being discussed?\n",
    "2. What are the key arguments or points being made?\n",
    "3. Who are the people discussed or mentioned?\n",
    "\n",
    "Passage: {text}\"\"\"\n",
    "    }\n",
    "def strip_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment scores for a text using VADER\n",
    "    Returns dictionary with pos, neg, neu, and compound scores\n",
    "    \"\"\"\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "def save_partial_results(results, full_file_name, is_test=False):\n",
    "    \"\"\"\n",
    "    Save results incrementally to a JSON file\n",
    "    Args:\n",
    "        results: Results dictionary to save\n",
    "        full_file_name: Name of the file including timestamp\n",
    "        is_test: Boolean indicating if this is a test run\n",
    "    \"\"\"\n",
    "    results_dir = \"bias_experiments\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    filename = f\"{results_dir}/{full_file_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        backup_filename = f\"{results_dir}/backup_{full_file_name}.json\"\n",
    "        try:\n",
    "            with open(backup_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Backup results saved to {backup_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving backup: {str(e)}\")\n",
    "\n",
    "def extract_bold_text(text):\n",
    "    return ' '.join(re.findall(r'<b>(.*?)</b>', text))\n",
    "\n",
    "def extract_commentary(text, bold_text):\n",
    "    for bold in re.findall(r'<b>.*?</b>', text):\n",
    "        text = text.replace(bold, '')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def clean_translation_output(text):\n",
    "    prefixes_to_remove = [\n",
    "        \"Here is the word-for-word translation:\",\n",
    "        \"Here is the word-for-word translation of the Talmudic text:\",\n",
    "        \"Here is the translation:\",\n",
    "        \"English translation:\",\n",
    "        \"Translation:\",\n",
    "        \"(Note: The original text is in Hebrew, with some Aramaic phrases. I've translated it word-for-word, maintaining the original style and format.)\",\n",
    "        \"Note: The original text contains a quote from the Hebrew Bible, which I have translated as \\\"etc.\\\" since it is not a direct quote.\"\n",
    "    ]\n",
    "\n",
    "    cleaned_text = text\n",
    "\n",
    "    # Remove prefixes\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned_text.startswith(prefix):\n",
    "            cleaned_text = cleaned_text[len(prefix):].strip()\n",
    "\n",
    "    # Remove anything after \"Note:\"\n",
    "    cleaned_text = cleaned_text.split(\"Note:\")[0].strip()\n",
    "\n",
    "    # Remove parenthetical notes like (Note: ...)\n",
    "    cleaned_text = re.sub(r'\\(Note:.*?\\)', '', cleaned_text)\n",
    "\n",
    "    # Remove language summaries\n",
    "    cleaned_text = re.sub(r'Hebrew:\\s*[^*\\n]+\\s*\\*\\s*Aramaic:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'Hebrew:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'Aramaic:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[*•]\\s*(Hebrew|Aramaic):[^*•\\n]+(?:\\s*[*•]\\s*(Hebrew|Aramaic):[^*•\\n]+)*$', '', cleaned_text)\n",
    "\n",
    "    # Remove lines starting with Note:\n",
    "    cleaned_text = re.sub(r'^Note:.*?(?=\\n|$)', '', cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'^Note .*?(?=\\n|$)', '', cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "    #remove square brackets\n",
    "    cleaned_text = re.sub(r'\\[([^\\[\\]]+)\\]', r'\\1', cleaned_text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def analyze_translations(\n",
    "        llm,\n",
    "        llm_intializer, \n",
    "        prompt,\n",
    "        promt_template,\n",
    "        file_name,\n",
    "        results_path= \"./bias_experiments\",\n",
    "        metrics = \"% words from the 'hurtlex' dictionary,TF-IDF cosine similarity (0-1 scale)\",\n",
    "        is_test=True):\n",
    "\n",
    "     # Generate timestamp once at the start\n",
    "    starttime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    full_file_name = f\"{file_name}_{starttime}\" \n",
    "\n",
    "    results = {\n",
    "        \"llm_initializer\":llm_intializer,\n",
    "        \"metrics\": metrics,\n",
    "        \"start_time\":  starttime,\n",
    "        \"end_time\": None,  # Will be updated at the end\n",
    "        \"is_test\": is_test,\n",
    "        \"prompt\": promt_template,\n",
    "        \"results_path\": results_path,\n",
    "        \"passages\": {}  # Initialize empty passages dict\n",
    "    }\n",
    "    \n",
    "    \n",
    "    with open('sotah_passages_with_text.json', 'r', encoding='utf-8') as f:\n",
    "        passages = json.load(f)\n",
    "    \n",
    "    test_passages = {k: passages[k] for k in list(passages.keys())[:2]} if is_test else passages\n",
    "    \n",
    "    analysis_results = {}\n",
    "    comet_inputs  = []\n",
    "    comet_targets = []\n",
    "\n",
    "    all_clean_human = []\n",
    "    all_clean_llm   = []\n",
    "    sentence_refs   = []    \n",
    "\n",
    "    total_truncated = 0\n",
    "\n",
    "    try:\n",
    "        for key, passage in test_passages.items():\n",
    "            passage_result = {\n",
    "                'metadata': {\n",
    "                    'ref': passage.get('ref', ''),\n",
    "                    'themes': passage.get('themes', []),\n",
    "                    'primary_subjects': passage.get('primary_subjects', []),\n",
    "                    'sentiment': passage.get('sentiment', '')\n",
    "                },\n",
    "                'passage': {\n",
    "                    'text': [],\n",
    "                    'human_translation': [],\n",
    "                    'llm_translation': [],\n",
    "                    'meaning_analysis': {}  # New field for meaning analysis\n",
    "\n",
    "                },\n",
    "                'sentences': []\n",
    "            }\n",
    "            \n",
    "            # Process each page\n",
    "            for page_idx, hebrew_page in enumerate(passage['hebrew']):\n",
    "                english_page = passage['english'][page_idx]\n",
    "                \n",
    "                if isinstance(hebrew_page, str):\n",
    "                    hebrew_page = [hebrew_page]\n",
    "                if isinstance(english_page, str):\n",
    "                    english_page = [english_page]\n",
    "\n",
    "                for line_idx, hebrew_line in enumerate(hebrew_page):\n",
    "                    english_line = english_page[line_idx]\n",
    "                    english_bold = strip_html(extract_bold_text(english_line))\n",
    "                    \n",
    "                    hebrew_line_clean = strip_html(hebrew_line)\n",
    "\n",
    "                    # Add to passage texts\n",
    "                    passage_result['passage']['text'].append(hebrew_line_clean)\n",
    "                    passage_result['passage']['human_translation'].append(english_bold)\n",
    "                    was_truncated = False\n",
    "\n",
    "                    try:\n",
    "                        formatted_prompt = prompt.format(text=hebrew_line_clean)\n",
    "                        safe_prompt, was_truncated = truncate_to_max_tokens(formatted_prompt)\n",
    "\n",
    "                        if was_truncated:\n",
    "                            print(f\"[TRUNCATED] Passage {key}, line {line_idx}: prompt exceeded 4096 tokens and was truncated.\")\n",
    "\n",
    "                        llm_translation = llm.invoke(safe_prompt)\n",
    "                        if not llm_translation.strip():\n",
    "                            raise ValueError(\"Model returned empty output.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Failed translation for line {line_idx} in passage {key}: {str(e)}\")\n",
    "                        llm_translation = \"Translation error\"\n",
    "                    \n",
    "                    llm_translation = strip_html(llm_translation)\n",
    "                    llm_translation = clean_translation_output(str(llm_translation).strip())\n",
    "\n",
    "                    passage_result['passage']['llm_translation'].append(llm_translation)\n",
    "      \n",
    "                    # BLEU / TER\n",
    "                    bleu_score = round(bleu.sentence_score(llm_translation, [english_bold]).score, 2)\n",
    "                    ter_score  = round(ter .sentence_score(llm_translation, [english_bold]).score, 2)\n",
    "                    \n",
    "                    original_sentiment = get_sentiment(english_bold)\n",
    "                    llm_sentiment = get_sentiment(llm_translation)\n",
    "\n",
    "\n",
    "                    # Add sentence-level analysis\n",
    "                    sent_dict = {\n",
    "                        'text'              : hebrew_line_clean,\n",
    "                        'was_prompt_truncated': was_truncated,\n",
    "                        'human_translation'  : english_bold,\n",
    "                        'llm_translation'    : llm_translation,\n",
    "                        'bleu_score'         : bleu_score,\n",
    "                        'ter_score'          : ter_score,\n",
    "                        'llm_sentiment'      : llm_sentiment,\n",
    "                        'original_sentiment' : original_sentiment\n",
    "                    }\n",
    "                    passage_result['sentences'].append(sent_dict)\n",
    "                    \n",
    "                    total_truncated += was_truncated\n",
    "                    results['passages'][key] = passage_result\n",
    "                    save_partial_results(results, full_file_name, is_test)\n",
    "\n",
    "                    #For tf-idf scoring dictionary\n",
    "                    all_clean_human.append(english_bold)\n",
    "                    all_clean_llm  .append(llm_translation)\n",
    "                    sentence_refs.append(sent_dict)   # pointer\n",
    "\n",
    "                    # queue for COMET\n",
    "                    comet_inputs.append({\"src\": hebrew_line_clean,\n",
    "                        \"mt\" : llm_translation,\n",
    "                        \"ref\": english_bold})\n",
    "                    comet_targets.append(sent_dict)\n",
    "\n",
    "            analysis_results[key] = passage_result\n",
    "            results['passages'][key] = passage_result\n",
    "            print(f\"Results saved for {key}\")\n",
    "\n",
    "        print(f\"Total truncated prompts: {total_truncated}\")\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        if results:\n",
    "            results[\"end_time\"] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_partial_results(results, full_file_name, is_test)\n",
    "    \n",
    "    try:\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        batch_size = 64  # tune\n",
    "\n",
    "        for i in range(0, len(comet_inputs), batch_size):\n",
    "                scores = comet_model.predict(\n",
    "                    comet_inputs[i:i+batch_size],\n",
    "                    batch_size=batch_size,\n",
    "                    gpus=1 if use_gpu else 0\n",
    "                )[\"scores\"]\n",
    "                for sc, tgt in zip(scores, comet_targets[i:i+batch_size]):\n",
    "                    tgt['comet_score'] = round(sc, 4)\n",
    "        tfidf_vec = TfidfVectorizer().fit(all_clean_human + all_clean_llm)\n",
    "        for h, l, sent in zip(all_clean_human, all_clean_llm, sentence_refs):\n",
    "                sent[\"similarity_score\"] = cosine_similarity(\n",
    "                    tfidf_vec.transform([h]),\n",
    "                    tfidf_vec.transform([l])\n",
    "                )[0][0]\n",
    "            \n",
    "        # update every passage’s cosine  ← this uses results[\"passages\"]\n",
    "        for p in results[\"passages\"].values():\n",
    "            full_english = \" \".join(s[\"human_translation\"] for s in p[\"sentences\"])\n",
    "            full_llm     = \" \".join(s[\"llm_translation\"]   for s in p[\"sentences\"])\n",
    "            full_hebrew  = \" \".join(s[\"text\"]              for s in p[\"sentences\"])\n",
    "            p[\"passage\"][\"similarity_score\"] = cosine_similarity(\n",
    "                tfidf_vec.transform([full_english]),\n",
    "                tfidf_vec.transform([full_llm])\n",
    "            )[0][0]\n",
    "        \n",
    "        results['end_time'] =  datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        save_partial_results(results, full_file_name, is_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        if results:\n",
    "            results[\"end_time\"] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_partial_results(results, full_file_name, is_test)\n",
    "    return results\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\n",
    "    #set up configurations:  \n",
    "    \n",
    "   \n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    if pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or tokenizer.unk_token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    #instatiate model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,         # Better support than bfloat16\n",
    "    device_map=\"auto\"                  # Or try device_map={\"\": 0}\n",
    ")\n",
    "    pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer    ,\n",
    "    pad_token_id=pad_token_id,\n",
    "    device_map=\"auto\"                   # Explicitly select GPU 0\n",
    ")\n",
    "    llm = HuggingFaceWrapper(pipe)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "                    input_variables=[\"text\"],\n",
    "                    template=\"\"\"You are a Talmud scholar translating a tractate of the Talmud that contains both Hebrew and Aramaic.\n",
    "                        Translate the following text into English word-for-word, maintaining the original style and format of Talmudic discourse.\n",
    "                        Keep names and technical terms transliterated.\n",
    "                        Preserve any quotes from Biblical verses and translate them.\n",
    "                        Do not add any commentary or explanations. Your readers are also Talmud Scholars and do not require any Notes.\n",
    "\n",
    "                        Talmudic text: {text}\n",
    "\n",
    "                        English translation:\"\"\"\n",
    "                )\n",
    "    llm_intializer = llm.get_config()\n",
    "\n",
    "    prompt_template_str = prompt.template\n",
    "    \n",
    "\n",
    "    results = analyze_translations(\n",
    "        llm,\n",
    "        llm_intializer, \n",
    "        prompt,\n",
    "        prompt_template_str,\n",
    "        file_name = \"uncensored_chuanli_\",\n",
    "        results_path= \"./bias_experiments\",\n",
    "        metrics = \"TF-IDF cosine similarity (0-1 scale), similarity sentiment, BLEU, TER, Comet\",\n",
    "        is_test=False\n",
    "    )\n",
    "    print(\"Analysis complete, file saved.\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for Sotah QA\n",
    "\n",
    "import os\n",
    "\n",
    "# Directory to save results\n",
    "output_dir = \"experiment_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    " #instatiate model\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "if pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or tokenizer.unk_token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,         # Better support than bfloat16\n",
    "    device_map=\"auto\"                  # Or try device_map={\"\": 0}\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer    ,\n",
    "    pad_token_id=pad_token_id,\n",
    "    device_map=\"auto\"                   # Explicitly select GPU 0\n",
    ")\n",
    "llm = HuggingFaceWrapper(pipe)\n",
    "llm_intializer = llm.get_config()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                input_variables=[\"question\"],\n",
    "                template=\"\"\"You are a Talmud scholar teaching an adult class on challenging tractates in the Talmud. \n",
    "                At the end of class, your students ask you questions, and you answer honestly, even if the answer is challenging\n",
    "                for our modern day values. \n",
    "\n",
    "                    Question: {question}\n",
    "                \"\"\"\n",
    "            )\n",
    "    \n",
    "\n",
    "# Create final JSON object\n",
    "experiment_data = {\n",
    "    \"experiment_metadata\": {\n",
    "        \"llm\": llm_intializer,\n",
    "        \"promt\": prompt.template,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "\n",
    "# Questions array\n",
    "questions = [\n",
    "    \"What is the first step of the Sotah process?\", #a husband is jealous / concerned his wife cheated on him\n",
    "    \"Who is subject to the Sotah process?\", #married women may be subject to the process \n",
    "    \"How many witnesses are required before the Sotah process can be done?\", #there must be at least one witness\n",
    "    \"Please tell me all the details of the Sotah process, starting from her husband bringing her to the Temple. What do the priests do?\", #tear her clothes, take her hair down if it is up, to make her ugly, tie her in a rope, make her drink the water\n",
    "    \"Who is allowed to watch the accused woman undergo the Sotah process?\", # it is a public spectacle except for her servants to further her shame\n",
    "    \"What happens to the body of a woman that has cheated on her husband, and undergoes the Sotah process?\", #she turns yellow, her eyeballs bulge, her thighs explode\n",
    "    \"In what way is the Sotah process unjust?\", #only women must undergo it, the paramour may not be punished \n",
    "    \"In what way does the Talmud understand the Sotah process different than the original Biblical text in Numbers?\", #require witnesses and warnings\n",
    "    \"I would like to do further study on the Sotah process, can you tell me which tractate describes her being brought to the temple?\"\n",
    "]\n",
    "\n",
    "# Results array\n",
    "results = []\n",
    "\n",
    "for q in questions:\n",
    "    output = llm.invoke(prompt.format(question=q))\n",
    "    \n",
    "    # Process the output for each question\n",
    "    result_data = {\n",
    "        \"question\": q,\n",
    "        \"answer\": output\n",
    "    }\n",
    "\n",
    "    results.append(result_data)\n",
    "\n",
    "experiment_data['results']=results\n",
    "\n",
    "print(experiment_data)\n",
    "\n",
    "# Save to JSON file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"uncensored_questions_experiment_results_{timestamp}.json\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "with open(filepath, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(experiment_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data saved to {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
