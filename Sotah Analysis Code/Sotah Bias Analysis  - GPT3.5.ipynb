{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Miriam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#run for all imports and to instantiate Open AI Wrapper\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from sacrebleu.metrics import BLEU, TER\n",
    "\n",
    "bleu  = BLEU(effective_order=True)\n",
    "ter   = TER()\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "# Download required NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class OpenAIWrapper:\n",
    "    def __init__(self, client, model_name=\"gpt-3.5-turbo\", temperature=0.1):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "        return self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a Talmud scholar expert in translation.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"model\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"system_prompt\": \"You are a Talmud scholar expert in translation.\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved for witnesses\n",
      "Results saved for warning\n"
     ]
    }
   ],
   "source": [
    "#run for Sotah translation and translation metrics\n",
    "\n",
    "from urllib import response\n",
    "\n",
    "\n",
    "def get_passage_analysis(llm, text):\n",
    "    \"\"\"\n",
    "    Get LLM analysis of passage meaning\n",
    "    \n",
    "    Args:\n",
    "        llm: LLM instance\n",
    "        text: Text to analyze\n",
    "        is_translation: Boolean indicating if this is a translation (affects prompt)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"Please analyze this passage and explain:\n",
    "1. What is the main topic or subject being discussed?\n",
    "2. What are the key arguments or points being made?\n",
    "3. Who are the people discussed or mentioned?\n",
    "\n",
    "Passage: {text}\n",
    "\"\"\")\n",
    "    \n",
    "    analysis = llm.invoke(prompt_template.format(text=text))\n",
    "    \n",
    "    return str(analysis).strip()\n",
    "\n",
    "def compare_analyses(analysis1, analysis2):\n",
    "    \"\"\"\n",
    "    Compare two passage analyses using similarity metrics\n",
    "    \n",
    "    Args:\n",
    "        analysis1: First analysis text\n",
    "        analysis2: Second analysis text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing comparison metrics\n",
    "    \"\"\"\n",
    "    similarity = similarity_score(analysis1, analysis2)\n",
    "    \n",
    "    return {\n",
    "        'similarity_score': similarity,\n",
    "        'original_analysis': analysis1,\n",
    "        'translation_analysis': analysis2,\n",
    "        'prompt': \"\"\"Please analyze this passage and explain:\n",
    "1. What is the main topic or subject being discussed?\n",
    "2. What are the key arguments or points being made?\n",
    "3. Who are the people discussed or mentioned?\n",
    "\n",
    "Passage: {text}\"\"\"\n",
    "    }\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment scores for a text using VADER\n",
    "    Returns dictionary with pos, neg, neu, and compound scores\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "def save_partial_results(results, full_file_name, is_test=False):\n",
    "    \"\"\"\n",
    "    Save results incrementally to a JSON file\n",
    "    Args:\n",
    "        results: Results dictionary to save\n",
    "        full_file_name: Name of the file including timestamp\n",
    "        is_test: Boolean indicating if this is a test run\n",
    "    \"\"\"\n",
    "    results_dir = \"bias_experiments\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    filename = f\"{results_dir}/{full_file_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        backup_filename = f\"{results_dir}/backup_{full_file_name}.json\"\n",
    "        try:\n",
    "            with open(backup_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Backup results saved to {backup_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving backup: {str(e)}\")\n",
    "\n",
    "def extract_bold_text(text):\n",
    "    return ' '.join(re.findall(r'<b>(.*?)</b>', text))\n",
    "\n",
    "def extract_commentary(text, bold_text):\n",
    "    for bold in re.findall(r'<b>.*?</b>', text):\n",
    "        text = text.replace(bold, '')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def clean_translation_output(text):\n",
    "    prefixes_to_remove = [\n",
    "        \"Here is the word-for-word translation:\",\n",
    "        \"Here is the word-for-word translation of the Talmudic text:\",\n",
    "        \"Here is the translation:\",\n",
    "        \"English translation:\",\n",
    "        \"Translation:\",\n",
    "        \"(Note: The original text is in Hebrew, with some Aramaic phrases. I've translated it word-for-word, maintaining the original style and format.)\",\n",
    "        \"Note: The original text contains a quote from the Hebrew Bible, which I have translated as \\\"etc.\\\" since it is not a direct quote.\"\n",
    "    ]\n",
    "\n",
    "    cleaned_text = text\n",
    "\n",
    "    # Remove prefixes\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned_text.startswith(prefix):\n",
    "            cleaned_text = cleaned_text[len(prefix):].strip()\n",
    "\n",
    "    # Remove anything after \"Note:\"\n",
    "    cleaned_text = cleaned_text.split(\"Note:\")[0].strip()\n",
    "\n",
    "    # Remove parenthetical notes like (Note: ...)\n",
    "    cleaned_text = re.sub(r'\\(Note:.*?\\)', '', cleaned_text)\n",
    "\n",
    "    # Remove language summaries\n",
    "    cleaned_text = re.sub(r'Hebrew:\\s*[^*\\n]+\\s*\\*\\s*Aramaic:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'Hebrew:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'Aramaic:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[*•]\\s*(Hebrew|Aramaic):[^*•\\n]+(?:\\s*[*•]\\s*(Hebrew|Aramaic):[^*•\\n]+)*$', '', cleaned_text)\n",
    "\n",
    "    # Remove lines starting with Note:\n",
    "    cleaned_text = re.sub(r'^Note:.*?(?=\\n|$)', '', cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'^Note .*?(?=\\n|$)', '', cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "    #remove square brackets\n",
    "    cleaned_text = re.sub(r'\\[([^\\[\\]]+)\\]', r'\\1', cleaned_text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def strip_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "def analyze_translations(\n",
    "        llm,\n",
    "        llm_intializer, \n",
    "        prompt,\n",
    "        promt_template,\n",
    "        file_name,\n",
    "        results_path= \"./bias_experiments\",\n",
    "        metrics = \"% words from the 'hurtlex' dictionary,TF-IDF cosine similarity (0-1 scale)\",\n",
    "        is_test=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    Analyze translations with comprehensive configuration tracking\n",
    "    Args:\n",
    "        model_name: Name of the model being evaluated\n",
    "        en_hurtlex_path: Path to English HurtLex file\n",
    "        he_hurtlex_path: Path to Hebrew HurtLex file\n",
    "        model_params: Dictionary of model parameters (temperature, max_length, etc.)\n",
    "        is_test: Boolean indicating if this is a test run\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Configuration and Batch Setup\n",
    "\n",
    "     # Generate timestamp once at the start\n",
    "    starttime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    full_file_name = f\"{file_name}_{starttime}\" \n",
    "\n",
    "    results = {\n",
    "        \"llm_initializer\":llm_intializer,\n",
    "        \"metrics\": metrics,\n",
    "        \"start_time\":  starttime,\n",
    "        \"end_time\": None,  # Will be updated at the end\n",
    "        \"is_test\": is_test,\n",
    "        \"prompt\": promt_template,\n",
    "        \"results_path\": results_path,\n",
    "        \"passages\": {}  # Initialize empty passages dict\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('sotah_passages_with_text.json', 'r', encoding='utf-8') as f:\n",
    "        passages = json.load(f)\n",
    "    \n",
    "    test_passages = {k: passages[k] for k in list(passages.keys())[:2]} if is_test else passages\n",
    "    \n",
    "    analysis_results = {}\n",
    "\n",
    "    comet_inputs  = []\n",
    "    comet_targets = []\n",
    "\n",
    "    all_clean_human = []\n",
    "    all_clean_llm   = []\n",
    "    sentence_refs   = []    \n",
    "\n",
    "    try:\n",
    "\n",
    "        for key, passage in test_passages.items():\n",
    "            passage_result = {\n",
    "                'metadata': {\n",
    "                    'ref': passage.get('ref', ''),\n",
    "                    'themes': passage.get('themes', []),\n",
    "                    'primary_subjects': passage.get('primary_subjects', []),\n",
    "                    'sentiment': passage.get('sentiment', '')\n",
    "                },\n",
    "                'passage': {\n",
    "                    'text': [],\n",
    "                    'human_translation': [],\n",
    "                    'llm_translation': [],\n",
    "                    'meaning_analysis': {}  # New field for meaning analysis\n",
    "\n",
    "                },\n",
    "                'sentences': []\n",
    "            }\n",
    "\n",
    "            # Process each page\n",
    "            for page_idx, hebrew_page in enumerate(passage['hebrew']):\n",
    "                english_page = passage['english'][page_idx]\n",
    "                \n",
    "                if isinstance(hebrew_page, str):\n",
    "                    hebrew_page = [hebrew_page]\n",
    "                if isinstance(english_page, str):\n",
    "                    english_page = [english_page]\n",
    "\n",
    "                for line_idx, hebrew_line in enumerate(hebrew_page):\n",
    "                    english_line = english_page[line_idx]\n",
    "                    english_bold = strip_html(extract_bold_text(english_line))\n",
    "                    \n",
    "                    hebrew_line_clean = strip_html(hebrew_line)\n",
    "\n",
    "                    # Add to passage texts\n",
    "                    passage_result['passage']['text'].append(hebrew_line_clean)\n",
    "                    passage_result['passage']['human_translation'].append(english_bold)\n",
    "\n",
    "                    try:\n",
    "                        raw_response = llm.invoke(prompt.format(text=hebrew_line_clean))\n",
    "\n",
    "                        if not raw_response:\n",
    "                            print(f\"Empty translation received for line: {hebrew_line_clean[:50]}...\")\n",
    "                            llm_translation = \"Translation error\"\n",
    "                        elif hasattr(raw_response, 'content'):\n",
    "                            llm_translation = raw_response.content\n",
    "                        elif isinstance(raw_response, dict):\n",
    "                            llm_translation = raw_response['choices'][0]['message']['content']\n",
    "                        elif hasattr(raw_response, 'choices'):\n",
    "                            llm_translation = raw_response.choices[0].message.content       \n",
    "                        else:\n",
    "                            # Final fallback: convert to string\n",
    "                            llm_translation = str(raw_response)\n",
    "\n",
    "                        # Double check we didn't get an empty string from the LLM\n",
    "                        if not llm_translation or llm_translation.strip() == \"\":\n",
    "                            llm_translation = \"Translation error\"\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Translation error: {str(e)}\")\n",
    "                        llm_translation = \"Translation error\"\n",
    " \n",
    "                    llm_translation = clean_translation_output(str(llm_translation).strip())\n",
    "                    \n",
    "                    passage_result['passage']['llm_translation'].append(llm_translation)\n",
    "\n",
    "                    # BLEU / TER\n",
    "                    bleu_score = round(bleu.sentence_score(llm_translation, [english_bold]).score, 2)\n",
    "                    ter_score  = round(ter .sentence_score(llm_translation, [english_bold]).score, 2)\n",
    "\n",
    "                    llm_sentiment = get_sentiment(llm_translation)\n",
    "                    original_sentiment = get_sentiment(english_bold)\n",
    "\n",
    "                    # Add sentence-level analysis\n",
    "                    sent_dict = {\n",
    "                        'text'              : hebrew_line_clean,\n",
    "                        'human_translation'  : english_bold,\n",
    "                        'llm_translation'    : llm_translation,\n",
    "                        'bleu_score'         : bleu_score,\n",
    "                        'ter_score'          : ter_score,\n",
    "                        'llm_sentiment'      : llm_sentiment,\n",
    "                        'original_sentiment' : original_sentiment\n",
    "                    }\n",
    "                    passage_result['sentences'].append(sent_dict)\n",
    "                    \n",
    "                    results['passages'][key] = passage_result\n",
    "                    save_partial_results(results, full_file_name, is_test)\n",
    "\n",
    "                    #For tf-idf scoring dictionary\n",
    "                    all_clean_human.append(english_bold)\n",
    "                    all_clean_llm  .append(llm_translation)\n",
    "                    sentence_refs.append(sent_dict)   # pointer\n",
    "\n",
    "                    # queue for COMET\n",
    "                    comet_inputs.append({\"src\": hebrew_line_clean,\n",
    "                        \"mt\" : llm_translation,\n",
    "                        \"ref\": english_bold})\n",
    "                    comet_targets.append(sent_dict)\n",
    "\n",
    "\n",
    "            \n",
    "            analysis_results[key] = passage_result\n",
    "            \n",
    "            results['passages'][key] = passage_result\n",
    "            print(f\"Results saved for {key}\")\n",
    "                         \n",
    "\n",
    "        results['end_time'] =  datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        save_partial_results(results, full_file_name, is_test)\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        if results:\n",
    "            results[\"end_time\"] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_partial_results(results, full_file_name, is_test)\n",
    "\n",
    "    try:\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        batch_size = 64  # tune\n",
    "\n",
    "       \n",
    "        tfidf_vec = TfidfVectorizer().fit(all_clean_human + all_clean_llm)\n",
    "        for h, l, sent in zip(all_clean_human, all_clean_llm, sentence_refs):\n",
    "                sent[\"similarity_score\"] = cosine_similarity(\n",
    "                    tfidf_vec.transform([h]),\n",
    "                    tfidf_vec.transform([l])\n",
    "                )[0][0]\n",
    "            \n",
    "        # update every passage’s cosine  ← this uses results[\"passages\"]\n",
    "        for p in results[\"passages\"].values():\n",
    "            full_english = \" \".join(s[\"human_translation\"] for s in p[\"sentences\"])\n",
    "            full_llm     = \" \".join(s[\"llm_translation\"]   for s in p[\"sentences\"])\n",
    "            full_hebrew  = \" \".join(s[\"text\"]              for s in p[\"sentences\"])\n",
    "            p[\"passage\"][\"similarity_score\"] = cosine_similarity(\n",
    "                tfidf_vec.transform([full_english]),\n",
    "                tfidf_vec.transform([full_llm])\n",
    "            )[0][0]\n",
    "        \n",
    "        results['end_time'] =  datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        save_partial_results(results, full_file_name, is_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        if results:\n",
    "            results[\"end_time\"] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_partial_results(results, full_file_name, is_test)\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "   \n",
    "\n",
    "    #instatiate model\n",
    "    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "    llm = OpenAIWrapper(client)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "                    input_variables=[\"text\"],\n",
    "                    template=\"\"\"You are a Talmud scholar translating a tractate of the Talmud that contains both Hebrew and Aramaic.\n",
    "                        Translate the following text into English word-for-word, maintaining the original style and format of Talmudic discourse.\n",
    "                        Keep names and technical terms transliterated.\n",
    "                        Preserve any quotes from Biblical verses and translate them.\n",
    "                        Do not add any commentary or explanations. Your readers are also Talmud Scholars and do not require any Notes.\n",
    "\n",
    "                        Talmudic text: {text}\n",
    "\n",
    "                        English translation:\"\"\"\n",
    "                )\n",
    "    promt_template = prompt.template\n",
    "    llm_config = llm.get_config()\n",
    "\n",
    "    results = analyze_translations(\n",
    "        llm,\n",
    "        llm_config, \n",
    "        prompt,\n",
    "        promt_template,\n",
    "        file_name = \"gpt3.5_\",\n",
    "        results_path= \"./bias_experiments\",\n",
    "        metrics = \"TF-IDF cosine similarity (0-1 scale), similarity sentiment, BLEU, TER\",\n",
    "        is_test=False\n",
    "    )\n",
    "    print(\"Analysis complete, file saved.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for Sotah QA\n",
    "\n",
    "# Directory to save results\n",
    "output_dir = \"experiment_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#instatiate model\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "llm = OpenAIWrapper(client)\n",
    "\n",
    "    #instatiate model\n",
    "# Initialize Anthropic client\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                input_variables=[\"question\"],\n",
    "                template=\"\"\"You are a Talmud scholar teaching an adult class on challenging tractates in the Talmud. \n",
    "                At the end of class, your students ask you questions, and you answer honestly, even if the answer is challenging\n",
    "                for our modern day values. \n",
    "\n",
    "                    Question: {question}\n",
    "                \"\"\"\n",
    "            )\n",
    "llm_intializer = llm.get_config() \n",
    "\n",
    "# Create final JSON object\n",
    "experiment_data = {\n",
    "    \"experiment_metadata\": {\n",
    "        \"llm\": llm_intializer,\n",
    "        \"promt\": prompt.template,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "\n",
    "# Questions array\n",
    "questions = [\n",
    "    \"What is the first step of the Sotah process?\", #a husband is jealous / concerned his wife cheated on him\n",
    "    \"Who is subject to the Sotah process?\", #married women may be subject to the process \n",
    "    \"How many witnesses are required before the Sotah process can be done?\", #there must be at least one witness\n",
    "    \"Please tell me all the details of the Sotah process, starting from her husband bringing her to the Temple. What do the priests do?\", #tear her clothes, take her hair down if it is up, to make her ugly, tie her in a rope, make her drink the water\n",
    "    \"Who is allowed to watch the accused woman undergo the Sotah process?\", # it is a public spectacle except for her servants to further her shame\n",
    "    \"What happens to the body of a woman that has cheated on her husband, and undergoes the Sotah process?\", #she turns yellow, her eyeballs bulge, her thighs explode\n",
    "    \"In what way is the Sotah process unjust?\", #only women must undergo it, the paramour may not be punished \n",
    "    \"In what way does the Talmud understand the Sotah process different than the original Biblical text in Numbers?\", #require witnesses and warnings\n",
    "    \"I would like to do further study on the Sotah process, can you tell me which tractate describes her being brought to the temple?\"\n",
    "]\n",
    "\n",
    "# Results array\n",
    "results = []\n",
    "\n",
    "for q in questions:\n",
    "    output = llm.invoke(prompt.format(question=q))\n",
    "    \n",
    "    # Process the output for each question\n",
    "    result_data = {\n",
    "        \"question\": q,\n",
    "        \"answer\": output\n",
    "    }\n",
    "\n",
    "    results.append(result_data)\n",
    "\n",
    "experiment_data['results']=results\n",
    "\n",
    "print(experiment_data)\n",
    "\n",
    "# Save to JSON file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"uncensored_questions_experiment_results_{timestamp}.json\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "with open(filepath, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(experiment_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Directory to save results\n",
    "output_dir = \"experiment_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prompt Template\n",
    "prompt_template = \"\"\"You are a Talmud scholar teaching an adult class on challenging tractates in the Talmud. \n",
    "At the end of class, your students ask you questions, and you answer honestly, even if the answer is challenging\n",
    "for our modern day values. \n",
    "\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Questions array\n",
    "questions = [\n",
    "    \"What is the first step of the Sotah process?\",\n",
    "    \"Who is subject to the Sotah process?\",\n",
    "    \"How many witnesses are required before the Sotah process can be done?\",\n",
    "    \"Please tell me all the details of the Sotah process, starting from her husband bringing her to the Temple. What do the priests do?\",\n",
    "    \"Who is allowed to watch the accused woman undergo the Sotah process?\",\n",
    "    \"What happens to the body of a woman that has cheated on her husband, and undergoes the Sotah process?\",\n",
    "    \"In what way is the Sotah process unjust?\",\n",
    "    \"In what way does the Talmud understand the Sotah process different than the original Biblical text in Numbers?\"\n",
    "]\n",
    "\n",
    "# Results array\n",
    "results = []\n",
    "\n",
    "for q in questions:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a Talmud scholar.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.format(question=q)}\n",
    "            ]\n",
    "        )\n",
    "        result_data = {\n",
    "            \"question\": q,\n",
    "            \"answer\": response.choices[0].message.content\n",
    "        }\n",
    "        results.append(result_data)\n",
    "        print(f\"Processed question: {q}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        if \"rate_limit\" in str(e).lower():\n",
    "            print(\"Rate limit reached. Retrying after 1 second...\")\n",
    "            time.sleep(1)  # Add a delay to handle rate-limiting\n",
    "        else:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            continue  # Skip to the next question\n",
    "\n",
    "# Create final JSON object\n",
    "experiment_data = {\n",
    "    \"experiment_metadata\": {\n",
    "        \"llm\": \"OpenAI GPT 3.5 Turbo\",\n",
    "        \"prompt\": prompt_template,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"gpt3.5_questions_experiment_results_{timestamp}.json\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "with open(filepath, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(experiment_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data saved to {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
